# -*- coding: utf-8 -*-
"""Ass4_COL774.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LIPLsdOs8PtUinbdmL_QEfE6pUe10Dzo
"""

import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import Normalizer,StandardScaler
from math import sqrt
import os
import pandas as pd
import matplotlib.pyplot as plt
import time
from sklearn.neural_network import MLPClassifier

class NeuralNetwork:

  def __init__(self,mini_batch_size,num_features,hidden_layers,target_classes):
    
    self.mb_size = mini_batch_size
    self.num_features = num_features
    self.weights = []
    self.bias = []
    hidden_layers.append(target_classes)
    temp_param = 0.2*np.random.random(size = (num_features,hidden_layers[0])) - 0.1
    self.weights.append(temp_param)
    self.bias.append(0.2*np.random.random(size = (hidden_layers[0],)) - 0.1)
    for i in range(len(hidden_layers) - 1):
       temp_param = 0.2*np.random.random(size = (hidden_layers[i],hidden_layers[i+1])) - 0.1
       self.weights.append(temp_param)
       self.bias.append(0.2*np.random.random(size = (hidden_layers[i+1],)) - 0.1)
    
    self.target_classes = target_classes

  def printData(self):

    for i in range(len(self.weights)):
      print("Size of weights {} is {}".format(i,self.weights[i].shape))
      print("Size of bias {} is {}".format(i,self.bias[i].shape))

  def forward(self,X):
    self.values = []
    self.values.append(X)
    for i in range(len(self.weights)):
      weight = self.weights[i]
      bias = self.bias[i]
      out_temp = np.matmul(X,weight) + bias
      out_activated = 1/(1 + np.exp(-out_temp))
      # print("---------WEIGHT----------")
      # print(weight)
      # print("---------BIAS----------")
      # print(bias)
      # print("---------OUTPUT----------")
      # print(out_activated)
      self.values.append(out_activated)
      X = out_activated

    return X

  def backprop(self,Y):

    labels = np.zeros((Y.shape[0],self.target_classes))
    rows = Y.shape[0]
    for i in range(rows):
      labels[i,int(Y[i,0])] = 1
    
    layer_grads = []
    weight_grads = []
    bias_grads = []
    num_layers = len(self.values)
    predictions = self.values[num_layers - 1]
    out_grad = (labels - predictions)*predictions*(1.0 - predictions)
    layer_grads.append(out_grad)
    curr_grad = out_grad
    for i in range(len(self.weights)- 1,0,-1):
      weights = self.weights[i]
      values = self.values[i]
      temp_grad = np.matmul(curr_grad,weights.T)*values*(1 - values)
      layer_grads.append(temp_grad)
      weight_grads.append(np.matmul(values.T,curr_grad)/rows)
      bias_grads.append(np.sum(curr_grad,axis = 0)/rows)
      curr_grad = temp_grad

    weights = self.weights[0]
    values = self.values[0]
    weight_grads.append(np.matmul(values.T,curr_grad)/rows)
    bias_grads.append(np.sum(curr_grad,axis = 0)/rows)

    weight_grads_out = []
    bias_grads_out = []

    while(len(weight_grads) > 0):
      temp_grad = weight_grads.pop()
      weight_grads_out.append(temp_grad)
      temp_grad = bias_grads.pop()
      bias_grads_out.append(temp_grad)
    
    return weight_grads_out,bias_grads_out

  def getLoss(self,Y):
    labels = np.zeros((Y.shape[0],self.target_classes))
    rows = Y.shape[0]
    for i in range(rows):
      labels[i,int(Y[i,0])] = 1
    num_layers = len(self.values)
    predictions = self.values[num_layers - 1]

    loss = np.sum(((predictions - labels)**2))/(2*rows)
    return loss

  def train(self,X,Y,lr = 0.1):
    (rows,columns) = X.shape
    batch_size = self.mb_size
    batches = int(rows/batch_size)
    if( rows % batch_size != 0):
      batches += 1   
    
    count = 0
    prev_loss = 0.0
    stopping_count = 0
    for j in range(10000):
      epoch_loss = 0.0
      for i in range(batches):
         start = i*batch_size
         end = min(start + batch_size,rows)
         X_batch = X[start:end]
         Y_batch = Y[start:end]
         probs = self.forward(X_batch)
         wg,bg = self.backprop(Y_batch)
         loss = self.getLoss(Y_batch)
         epoch_loss += loss
         preds = np.argmax(probs,axis = 1)
         acc_score = accuracy_score(Y_batch,preds)
        #  if(count % 1000 == 0):
        #   print("Loss for iteration {} is {} and training accuracy is {}%".format(count,loss,acc_score*100))
         count += 1
         for k in range(len(wg)):
           weight_grad = wg[k]
           bias_grad = bg[k]
           self.weights[k] += lr*weight_grad
           self.bias[k] += lr*bias_grad
      
      print("Loss for iteration {} is {}".format(j,epoch_loss))
      if((abs(prev_loss - epoch_loss)/epoch_loss) > 0.0005):
        stopping_count = 0
      else:
        stopping_count += 1
        if(stopping_count > 10):
          print("Total epochs are {}".format(j))
          break
      prev_loss = epoch_loss
      # probs_test = NN.forward(X_test)
      # preds_test = np.argmax(probs_test,axis = 1)
      # acc_score = accuracy_score(Y_test,preds_test)
      # print("Test accuracy is {}% after {} epochs".format(100*acc_score,j))

    print("Loss function did not converge and 10000 iterations were reached..")

      
  def train_adaptive(self,X,Y,lr = 0.5):
    (rows,columns) = X.shape
    batch_size = self.mb_size
    batches = int(rows/batch_size)
    if( rows % batch_size != 0):
      batches += 1   
    
    count = 0
    prev_loss = 0.0
    stopping_count = 0
    for j in range(10000):
      epoch_loss = 0.0
      for i in range(batches):
         start = i*batch_size
         end = min(start + batch_size,rows)
         X_batch = X[start:end]
         Y_batch = Y[start:end]
         probs = self.forward(X_batch)
         wg,bg = self.backprop(Y_batch)
         loss = self.getLoss(Y_batch)
         epoch_loss += loss
         preds = np.argmax(probs,axis = 1)
         acc_score = accuracy_score(Y_batch,preds)
        #  if(count % 1000 == 0):
        #   print("Loss for iteration {} is {} and training accuracy is {}%".format(count,loss,acc_score*100))
         count += 1
         rate = lr/(sqrt(j + 1))
         for k in range(len(wg)):
           weight_grad = wg[k]
           bias_grad = bg[k]
           self.weights[k] += rate*weight_grad
           self.bias[k] += rate*bias_grad
      
      print("Loss for iteration {} is {}".format(j,epoch_loss))
      if((abs(prev_loss - epoch_loss)/epoch_loss) > 0.00005):
        stopping_count = 0
      else:
        stopping_count += 1
        if(stopping_count > 10):
          print("Total epochs are {}".format(j))
          break
      prev_loss = epoch_loss
    print("Loss function did not converge and 10000 iterations were reached..")


class NeuralNetwork_RELU:

  def __init__(self, mini_batch_size, num_features, hidden_layers, target_classes):

    self.mb_size = mini_batch_size
    self.num_features = num_features
    self.weights = []
    self.bias = []
    hidden_layers.append(target_classes)
    temp_param = 0.2 * np.random.random(size=(num_features, hidden_layers[0])) - 0.1
    self.weights.append(temp_param)
    self.bias.append(0.2 * np.random.random(size=(hidden_layers[0],)) - 0.1)
    for i in range(len(hidden_layers) - 1):
      temp_param = 0.2 * np.random.random(size=(hidden_layers[i], hidden_layers[i + 1])) - 0.1
      self.weights.append(temp_param)
      self.bias.append(0.2 * np.random.random(size=(hidden_layers[i + 1],)) - 0.1)

    self.target_classes = target_classes

  def printData(self):

    for i in range(len(self.weights)):
      print("Size of weights {} is {}".format(i, self.weights[i].shape))
      print("Size of bias {} is {}".format(i, self.bias[i].shape))

  def forward(self, X):
    self.values = []
    self.values.append(X)
    for i in range(len(self.weights) - 1):
      weight = self.weights[i]
      bias = self.bias[i]
      out_temp = np.matmul(X, weight) + bias
      out_activated = out_temp * (out_temp > 0)
      # print("---------WEIGHT----------")
      # print(weight)
      # print("---------BIAS----------")
      # print(bias)
      # print("---------OUTPUT----------")
      # print(out_activated)
      self.values.append(out_activated)
      X = out_activated

    weight = self.weights[len(self.weights) - 1]
    bias = self.bias[len(self.weights) - 1]
    out_temp = np.matmul(X, weight) + bias
    out_activated = 1 / (1 + np.exp(-out_temp))
    # print("---------WEIGHT----------")
    # print(weight)
    # print("---------BIAS----------")
    # print(bias)
    # print("---------OUTPUT----------")
    # print(out_activated)
    self.values.append(out_activated)
    X = out_activated

    return X

  def backprop(self, Y):

    labels = np.zeros((Y.shape[0], self.target_classes))
    rows = Y.shape[0]
    for i in range(rows):
      labels[i, int(Y[i, 0])] = 1

    layer_grads = []
    weight_grads = []
    bias_grads = []
    num_layers = len(self.values)
    predictions = self.values[num_layers - 1]
    out_grad = (labels - predictions) * predictions * (1.0 - predictions)
    layer_grads.append(out_grad)
    curr_grad = out_grad
    for i in range(len(self.weights) - 1, 0, -1):
      weights = self.weights[i]
      values = self.values[i]
      deriv = values > 0
      temp_grad = np.matmul(curr_grad, weights.T) * deriv
      layer_grads.append(temp_grad)
      weight_grads.append(np.matmul(values.T, curr_grad) / rows)
      bias_grads.append(np.sum(curr_grad, axis=0) / rows)
      curr_grad = temp_grad

    weights = self.weights[0]
    values = self.values[0]
    weight_grads.append(np.matmul(values.T, curr_grad) / rows)
    bias_grads.append(np.sum(curr_grad, axis=0) / rows)

    weight_grads_out = []
    bias_grads_out = []

    while (len(weight_grads) > 0):
      temp_grad = weight_grads.pop()
      weight_grads_out.append(temp_grad)
      temp_grad = bias_grads.pop()
      bias_grads_out.append(temp_grad)

    return weight_grads_out, bias_grads_out

  def getLoss(self, Y):
    labels = np.zeros((Y.shape[0], self.target_classes))
    rows = Y.shape[0]
    for i in range(rows):
      labels[i, int(Y[i, 0])] = 1
    num_layers = len(self.values)
    predictions = self.values[num_layers - 1]

    loss = np.sum(((predictions - labels) ** 2)) / (2 * rows)
    return loss


  def train_adaptive(self, X, Y, lr=0.5):
    (rows, columns) = X.shape
    batch_size = self.mb_size
    batches = int(rows / batch_size)
    if (rows % batch_size != 0):
      batches += 1

    count = 0
    prev_loss = 0.0
    stopping_count = 0
    for j in range(10000):
      epoch_loss = 0.0
      for i in range(batches):
        start = i * batch_size
        end = min(start + batch_size, rows)
        X_batch = X[start:end]
        Y_batch = Y[start:end]
        probs = self.forward(X_batch)
        wg, bg = self.backprop(Y_batch)
        loss = self.getLoss(Y_batch)
        epoch_loss += loss
        preds = np.argmax(probs, axis=1)
        acc_score = accuracy_score(Y_batch, preds)
        #  if(count % 1000 == 0):
        #   print("Loss for iteration {} is {} and training accuracy is {}%".format(count,loss,acc_score*100))
        count += 1
        rate = lr / (sqrt(j + 1))
        for k in range(len(wg)):
          weight_grad = wg[k]
          bias_grad = bg[k]
          self.weights[k] += rate * weight_grad
          self.bias[k] += rate * bias_grad

      print("Loss for iteration {} is {}".format(j, epoch_loss))
      if ((abs(prev_loss - epoch_loss) / epoch_loss) > 0.00005):
        stopping_count = 0
      else:
        stopping_count += 1
        if (stopping_count > 10):
          print("Total epochs are {}".format(j))
          break
      prev_loss = epoch_loss


def getOneHot(Y, target_classes):
  labels = np.zeros((Y.shape[0], target_classes))
  rows = Y.shape[0]
  for i in range(rows):
    labels[i, int(Y[i, 0])] = 1

  return labels



#############################   MAIN PROGRAM ################################################



base_dir = "D:\IIT Assignments\COL774\Assign 3(b)"
train = pd.read_csv(os.path.join(base_dir,"train.csv"),sep = ",",header = None).to_numpy()
test = pd.read_csv(os.path.join(base_dir,"test.csv"),sep = ",",header= None).to_numpy()

X_train = train[:,:-1]
Y_train = train[:,-1:]
X_test = test[:,:-1]
Y_test = test[:,-1:]

norm = StandardScaler()
X_train = norm.fit_transform(X_train)
X_test = norm.transform(X_test)


NN = NeuralNetwork(100,784,[10],26)
NN.printData()
print("Running Neural Network with constant rate for hidden layer size = 10...")
NN.train(X_train,Y_train)

probs = NN.forward(X_train)
preds = np.argmax(probs,axis = 1)
acc_score = accuracy_score(Y_train,preds)
print("Training accuracy is {}".format(acc_score))

probs_test = NN.forward(X_test)
preds_test = np.argmax(probs_test,axis = 1)
acc_score = accuracy_score(Y_test,preds_test)
print("Test accuracy is {}".format(acc_score))


counts = [1,5,10,50,100]

train_accuracies = []
test_accuracies = []
time_taken = []
for count in counts:
  print("Running for number of neurons in hidden layer = {}".format(count))
  NN = NeuralNetwork(100,784,[count],26)

  start = time.time()
  NN.train(X_train,Y_train,lr = 0.1)
  end =time.time()
  time_taken.append(end - start)
  print("Time taken to run is {} seconds".format(end - start))
  probs = NN.forward(X_train)
  preds = np.argmax(probs,axis = 1)
  acc_score = accuracy_score(Y_train,preds)
  print("Training accuracy is {}".format(acc_score))
  train_accuracies.append(acc_score)
  probs_test = NN.forward(X_test)
  preds_test = np.argmax(probs_test,axis = 1)
  acc_score = accuracy_score(Y_test,preds_test)
  print("Test accuracy is {}".format(acc_score))
  print("\n\n\n")
  test_accuracies.append(acc_score)

plt.plot(counts,train_accuracies)
plt.title('Accuracy on training set')
plt.xlabel('Number of neurons')
plt.ylabel('Accuracy in %')
plt.title('Training Accuracy vs. NUmber of Neurons')
plt.savefig('Assign4_b_training_accuracy.png')
plt.show()

plt.plot(counts,test_accuracies)
plt.title('Accuracy on test set')
plt.xlabel('Number of neurons')
plt.ylabel('Accuracy in %')
plt.title('Test Accuracy vs. NUmber of Neurons')
plt.savefig('Assign4_b_test_accuracy.png')
plt.show()

plt.plot(counts,time_taken)
plt.title('Time taken for training')
plt.xlabel('Number of neurons')
plt.ylabel('Time taken(seconds)')
plt.title('Training Time vs. NUmber of Neurons')
plt.savefig('Assign4_b_time_taken.png')
plt.show()

counts = [1,5,10,50,100]

train_accuracies = []
test_accuracies = []
time_taken = []
for count in counts:
  print("Running for number of neurons in hidden layer = {}".format(count))
  NN = NeuralNetwork(100,784,[count],26)

  start = time.time()
  NN.train_adaptive(X_train,Y_train)
  end =time.time()
  time_taken.append(end - start)
  print("Time taken to run is {} seconds".format(end - start))
  probs = NN.forward(X_train)
  preds = np.argmax(probs,axis = 1)
  acc_score = accuracy_score(Y_train,preds)
  print("Training accuracy is {}".format(acc_score))
  train_accuracies.append(acc_score)
  probs_test = NN.forward(X_test)
  preds_test = np.argmax(probs_test,axis = 1)
  acc_score = accuracy_score(Y_test,preds_test)
  print("Test accuracy is {}".format(acc_score))
  print("\n\n\n")
  test_accuracies.append(acc_score)

plt.plot(counts,train_accuracies)
plt.title('Accuracy on training set')
plt.xlabel('Number of neurons')
plt.ylabel('Accuracy in %')
plt.title('Training Accuracy vs. NUmber of Neurons')
plt.savefig('Assign4_b_training_accuracy_adaptive.png')
plt.show()

plt.plot(counts,test_accuracies)
plt.title('Accuracy on test set')
plt.xlabel('Number of neurons')
plt.ylabel('Accuracy in %')
plt.title('Test Accuracy vs. NUmber of Neurons')
plt.savefig('Assign4_b_test_accuracy_adaptive.png')
plt.show()

plt.plot(counts,time_taken)
plt.title('Time taken for training')
plt.xlabel('Number of neurons')
plt.ylabel('Time taken(seconds)')
plt.title('Training Time vs. NUmber of Neurons')
plt.savefig('Assign4_b_time_taken_adaptive.png')
plt.show()

NN = NeuralNetwork(100,784,[100,100],26)
NN.printData()
NN.train_adaptive(X_train,Y_train)
probs = NN.forward(X_train)
preds = np.argmax(probs,axis = 1)
acc_score = accuracy_score(Y_train,preds)
print("Training accuracy is {}".format(acc_score))
probs_test = NN.forward(X_test)
preds_test = np.argmax(probs_test,axis = 1)
acc_score = accuracy_score(Y_test,preds_test)
print("Test accuracy is {}".format(acc_score))
print("\n\n\n")


NN = NeuralNetwork_RELU(100,784,[100,100],26)
NN.printData()
NN.train_adaptive(X_train,Y_train)
probs = NN.forward(X_train)
preds = np.argmax(probs,axis = 1)
acc_score = accuracy_score(Y_train,preds)
print("Training accuracy is {}".format(acc_score))
probs_test = NN.forward(X_test)
preds_test = np.argmax(probs_test,axis = 1)
acc_score = accuracy_score(Y_test,preds_test)
print("Test accuracy is {}".format(acc_score))
print("\n\n\n")


print("Running using Sklearn implementation of multilater neural network.....")


clf = MLPClassifier(hidden_layer_sizes= (100,100,),solver = "sgd" , alpha = 0.1,max_iter = 2000)
Y_one_hot_train = getOneHot(Y_train,26)
start = time.time()
clf.fit(X_train,Y_one_hot_train)
end = time.time()
print("Time taken is {}".format(end - start))
probs = clf.predict_proba(X_train)
preds = np.argmax(probs,axis = 1)
acc_score = accuracy_score(Y_train,preds)
print("Training accuracy is {}".format(acc_score))

probs_test = clf.predict_proba(X_test)
preds_test = np.argmax(probs_test,axis = 1)
acc_score = accuracy_score(Y_test,preds_test)
print("Test accuracy is {}".format(acc_score))
print("\n\n\n")

